{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task : Identifying the number of duplicate questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Training and test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import jaccard_score\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('corpusTrain.csv')\n",
    "data_test = pd.read_csv('corpusTest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(dataset.Content[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(dataset.Content[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = dataset.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtest = data_test.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtest = qtest.apply(lambda x : (re.sub('[^a-zA-Z]', ' ', x)).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = questions.apply(lambda x : (re.sub('[^a-zA-Z]', ' ', x)).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "vectorizer.fit(questions)\n",
    "sparse=vectorizer.transform(questions)\n",
    "sparse_test=vectorizer.transform(qtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "memory error occured at 100k after 10 k ran smoothly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now i try to split the sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.arange(0,51).tolist()\n",
    "y = [k*10000 for k in y]\n",
    "y.append(sparse.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countn = 0\n",
    "t_startn = time.time()\n",
    "for t in range(sparse_test.shape[0]):\n",
    "    for k in range(len(y)-1):    \n",
    "        cm=cosine_similarity(sparse_test[t],sparse[y[k]:y[k+1]])\n",
    "        if np.max(cm) > 0.8:\n",
    "            countn += 1\n",
    "            break\n",
    "query_time1 = time.time() - t_startn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_time1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implementing jaccard similarity\n",
    "consider the fact that jaccard_similarity = 1 - jaccard distance metric\n",
    "since the pairwise implementation for the jaccard metric cannot take sparse matrices, we had to convert to numpy and to do this\n",
    "we needed a smaller array size so we reduced the feature size of the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer1 = TfidfVectorizer()\n",
    "sparse1=vectorizer1.fit_transform(questions)\n",
    "sparse_test1=vectorizer1.transform(qtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = sparse_test1.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1=np.arange(0,51).tolist()\n",
    "y1 = [k*1000 for k in y1]\n",
    "y1.append(sparse1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "countj = 0\n",
    "t_startj = time.time()\n",
    "for t in range(sparse_test1.shape[0]):\n",
    "    print(t)\n",
    "    for k in range(len(y1)-1):  \n",
    "        spt = sparse1[y1[k]:y1[k+1]].todense()\n",
    "        dist = pairwise_distances(sp[t],spt,metric='jaccard')\n",
    "        if 1 - np.min(dist) > 0.8:\n",
    "            countj += 1\n",
    "            break\n",
    "query_timej = time.time() - t_startj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deducting wasted time\n",
    "tc=time.time()\n",
    "print(1)\n",
    "ttp=time.time()-tc\n",
    "tcc=time.time()\n",
    "spk = sparse[:10000].todense()\n",
    "ttd = time.time()-tcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('query time is : {} hours'.format((query_timej - sparse_test.shape[0]*ttp - sparse_test.shape[0]*len(y)*ttd)/3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implementing the datasketch library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import MinHash, MinHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qsplit = qtest.apply(lambda x : set(x.split()))\n",
    "questions_split = questions.apply(lambda x : set(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init=time.time()\n",
    "#minhash table for trainx\n",
    "mhashes2 =[]\n",
    "for i in range(questions_split.shape[0]):\n",
    "    mhashes2.append(MinHash(num_perm=16))\n",
    "    for d in questions_split[i]:\n",
    "        mhashes2[i].update(d.encode('utf8'))\n",
    "#creating lsh index\n",
    "lsh = MinHashLSH(threshold=0.8, num_perm=16)\n",
    "for i in range(questions_split.shape[0]):\n",
    "    lsh.insert('t'+str(i), mhashes2[i])\n",
    "Build_time=time.time() - init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Build_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_num = 0\n",
    "init1=time.time()\n",
    "#minhash index for test set\n",
    "mhashes =[]\n",
    "for i in range(qsplit.shape[0]):\n",
    "    mhashes.append(MinHash(num_perm=16))\n",
    "    for d in qsplit[i]:\n",
    "        mhashes[i].update(d.encode('utf8'))\n",
    "for i in range(qsplit.shape[0]):\n",
    "    if len(lsh.query(mhashes[i])):\n",
    "        count_num += 1\n",
    "q_time=time.time() - init1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n permutations = 32\n",
    "init=time.time()\n",
    "#minhash table for trainx\n",
    "mhashes2 =[]\n",
    "for i in range(questions_split.shape[0]):\n",
    "    mhashes2.append(MinHash(num_perm=32))\n",
    "    for d in questions_split[i]:\n",
    "        mhashes2[i].update(d.encode('utf8'))\n",
    "#creating lsh index\n",
    "lsh1 = MinHashLSH(threshold=0.8, num_perm=32)\n",
    "for i in range(questions_split.shape[0]):\n",
    "    lsh1.insert('t'+str(i), mhashes2[i])\n",
    "Build_time32=time.time() - init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Build_time32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_num32 = 0\n",
    "init1=time.time()\n",
    "#minhash index for test set\n",
    "mhashes =[]\n",
    "for i in range(qsplit.shape[0]):\n",
    "    mhashes.append(MinHash(num_perm=32))\n",
    "    for d in qsplit[i]:\n",
    "        mhashes[i].update(d.encode('utf8'))\n",
    "for i in range(qsplit.shape[0]):\n",
    "    if len(lsh1.query(mhashes[i])):\n",
    "        count_num32 += 1\n",
    "q_time1=time.time() - init1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_time1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_num32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n permutations = 64\n",
    "init=time.time()\n",
    "#minhash table for trainx\n",
    "mhashes2 =[]\n",
    "for i in range(questions_split.shape[0]):\n",
    "    mhashes2.append(MinHash(num_perm=64))\n",
    "    for d in questions_split[i]:\n",
    "        mhashes2[i].update(d.encode('utf8'))\n",
    "#creating lsh index\n",
    "lsh2 = MinHashLSH(threshold=0.8, num_perm=64)\n",
    "for i in range(questions_split.shape[0]):\n",
    "    lsh2.insert('t'+str(i), mhashes2[i])\n",
    "Build_time64=time.time() - init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Build_time64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_num64 = 0\n",
    "init1=time.time()\n",
    "#minhash index for test set\n",
    "mhashes =[]\n",
    "for i in range(qsplit.shape[0]):\n",
    "    mhashes.append(MinHash(num_perm=64))\n",
    "    for d in qsplit[i]:\n",
    "        mhashes[i].update(d.encode('utf8'))\n",
    "for i in range(qsplit.shape[0]):\n",
    "    if len(lsh2.query(mhashes[i])):\n",
    "        count_num64 += 1\n",
    "q_time2=time.time() - init1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_time2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_num64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Random forest using single hash table. the accuracy here can be improved with the use of \n",
    "multiple hashtables. it is worth noting here that facing memory issueas here we also reduced the max_feature of the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = TfidfVectorizer()\n",
    "vectorizer2.fit(questions)\n",
    "sparse2=vectorizer2.transform(questions)\n",
    "sparse2_test=vectorizer2.transform(qtest)\n",
    "sp=sparse2\n",
    "spt=sparse2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashf(vects,proj):\n",
    "    h=vects.dot(proj.T)>0\n",
    "    return bool_int(h[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bool_int(b):   \n",
    "    y = 0\n",
    "    for i,j in enumerate(b):\n",
    "        if j: y += 1<<i\n",
    "    return y\n",
    "#bool_int([1,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import *\n",
    "import scipy\n",
    "class LSH:\n",
    "    \n",
    "    def __init__(self, hash_size, dim,thr):\n",
    "        self.table = dict()\n",
    "        self.thr = thr\n",
    "        self.hash_size = hash_size\n",
    "        self.projections = np.random.randn(self.hash_size, dim)\n",
    "\n",
    "    def insert(self, vecs):\n",
    "        h= hashf(vecs, self.projections)\n",
    "        if h in self.table.keys():\n",
    "            self.table[h] = scipy.sparse.vstack((self.table[h],vecs))\n",
    "        else:\n",
    "            self.table[h] = vecs\n",
    "\n",
    "    def query(self, vecs):\n",
    "        h = hashf(vecs, self.projections)\n",
    "        cm=cosine_similarity(vecs,self.table[h])\n",
    "        if np.max(cm)>self.thr:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh = LSH(2,sp.shape[1],0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_init = time.time()\n",
    "for i in range(sp.shape[0]):\n",
    "    lsh.insert(sp[i])\n",
    "    #print(i)\n",
    "#lsh.sparse()\n",
    "ent = time.time() - t_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hash distribution')\n",
    "for h in lsh.table.keys():\n",
    "    print('{}:{}'.format(h,lsh.table[h].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the search space has now been divided into four but as we can see , each segment is still pretty large so we do not expect \n",
    "query time to be small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntt=0\n",
    "qtt=time.time()\n",
    "for i in range(spt.shape[0]): \n",
    "    if lsh.query(spt[i]):\n",
    "        cntt+=1\n",
    "qte = time.time() - qtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for k=3\n",
    "lsh2 = LSH(3,sp.shape[1],0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_init2 = time.time()\n",
    "for i in range(sp.shape[0]):\n",
    "    lsh2.insert(sp[i])\n",
    "ent2 = time.time() - t_init2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hash distribution')\n",
    "for h in lsh2.table.keys():\n",
    "    print('{}:{}'.format(h,lsh2.table[h].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntt2=0\n",
    "qtt=time.time()\n",
    "for i in range(spt.shape[0]):\n",
    "    if lsh2.query(spt[i]):\n",
    "        cntt2+=1\n",
    "qte2 = time.time() - qtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qte2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntt2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
