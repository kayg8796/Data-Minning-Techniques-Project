{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('train3.csv')\n",
    "data_test = pd.read_csv('test_without_labels3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x='Label', data=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = dataset['Content']\n",
    "reviews_test = data_test['Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "stop = stopwords.words('english2')\n",
    "\n",
    "def lemmatize_text1(text):\n",
    "    return [lemmatizer.lemmatize(w,pos=\"v\") for w in w_tokenizer.tokenize(text)]\n",
    "def lemmatize_text2(text):\n",
    "    return [lemmatizer.lemmatize(w,pos=\"n\") for w in w_tokenizer.tokenize(text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article=pd.concat([reviews,reviews_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article= article.apply(lambda x: \" \".join(x.lower() for x in x.split())) #lowercase\n",
    "article= article.str.replace('[^\\w\\s]','') #remove punctuation\n",
    "article = article.str.replace('\\d+', '') #remove numbers \n",
    "article= article.apply(lemmatize_text1)\n",
    "article= article.apply(lambda x: \" \".join(x))\n",
    "article= article.apply(lemmatize_text2)\n",
    "article= article.apply(lambda x: \" \".join(x))\n",
    "article= article.apply(lambda x: \" \".join(x for x in x.split() if x not in stop)) #remove stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = dataset['Label']\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "xtrain = vectorizer.fit_transform(article[:25000])\n",
    "xtest = vectorizer.transform(article[25000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical ML method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report as clrp\n",
    "kf = KFold(n_splits=5)\n",
    "from sklearn.svm import LinearSVC\n",
    "classifier2 = LinearSVC(random_state=0, tol=1e-5)\n",
    "for train_index, test_index in kf.split(xtrain,ytrain):\n",
    "    classifier2.fit(xtrain[train_index],ytrain[train_index])\n",
    "    ypred=classifier2.predict(xtrain[test_index])\n",
    "    ytestt=ytrain[test_index]\n",
    "    print(clrp(ytestt,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2.fit(xtrain,ytrain)\n",
    "y_pred = classifier2.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Predicted']=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test=data_test.drop('Content',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv = data_test.to_csv (r'C:\\Users\\Sarah\\Desktop\\export_dataframe3.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.callbacks import EarlyStopping as stoppoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess text\n",
    "article=pd.concat([reviews,reviews_test])\n",
    "ytrain = dataset['Label']\n",
    "\n",
    "article= article.apply(lambda x: \" \".join(x.lower() for x in x.split())) #lowercase\n",
    "article= article.str.replace('[^\\w\\s]','') #remove punctuation\n",
    "article = article.str.replace('\\d+', '') #remove numbers \n",
    "article = article.str.replace(' br ', ' ') #remove breaks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(np.unique(np.hstack(article[:25000]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = dataset['Label']\n",
    "result = [len(x.split()) for x in article]\n",
    "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating embeddings\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(article[:25000])\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(article[:25000])\n",
    "X_test = tokenizer.texts_to_sequences(article[25000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding\n",
    "maxlen = 500\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "model = Sequential()\n",
    "model.add(Embedding(20000, 32, input_length=500))\n",
    "model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for train_index, test_index in kf.split(xtrain,ytrain):\n",
    "    model.fit(X_train[train_index], ytrain[train_index], epochs=1, batch_size=128, verbose=2)\n",
    "    ypred=model.predict(X_train[test_index])\n",
    "    yp = [int(round(i[0])) for i in ypred]\n",
    "    ytestt=ytrain[test_index]\n",
    "    print(clrp(ytestt,yp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, ytrain, epochs=1, batch_size=128, verbose=2)\n",
    "ypred=model.predict(X_test)\n",
    "yp = [int(round(i[0])) for i in ypred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame()\n",
    "predictions['Id'] = np.arange(0,X_test.shape[0])\n",
    "predictions['Predicted']=yp\n",
    "export_csv = predictions.to_csv (r'C:\\Users\\Sarah\\Desktop\\export_dataframe10.csv', index = None, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
