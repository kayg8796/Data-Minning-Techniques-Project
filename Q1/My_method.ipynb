{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "stop = stopwords.words('english3')\n",
    "\n",
    "def lemmatize_text1(text):\n",
    "    return [lemmatizer.lemmatize(w,pos=\"v\") for w in w_tokenizer.tokenize(text)]\n",
    "def lemmatize_text2(text):\n",
    "    return [lemmatizer.lemmatize(w,pos=\"n\") for w in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = pd.read_csv('train.csv')\n",
    "dataset2 = pd.read_csv('test.csv')\n",
    "xtrain1 = dataset1['Title']+dataset1['Content']\n",
    "xtrain2 = dataset2['Title']+dataset2['Content']\n",
    "article=pd.concat([xtrain1,xtrain2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "article= article.str.replace('[^\\w\\s]','') #remove punctuation\n",
    "article= article.str.replace('\\d+', '') #remove numbers \n",
    "article= article.apply(lambda x: \" \".join(x.lower() for x in x.split())) #lowercase\n",
    "article= article.apply(lemmatize_text1)\n",
    "article= article.apply(lemmatize_text2)\n",
    "article= article.apply(lambda x: \" \".join(x for x in x if x not in stop)) #remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(article[:111795])\n",
    "xtrain=vectors[:111795]\n",
    "ytrain=dataset1['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "ytrain_matrix=labelencoder.fit_transform(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report as clrp\n",
    "kf = KFold(n_splits=5)\n",
    "from sklearn.svm import LinearSVC\n",
    "classifier2 = LinearSVC(random_state=0, tol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      4995\n",
      "           1       0.99      0.99      0.99      8947\n",
      "           2       0.98      0.98      0.98      2358\n",
      "           3       0.96      0.97      0.97      6059\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     22359\n",
      "   macro avg       0.97      0.97      0.97     22359\n",
      "weighted avg       0.97      0.97      0.97     22359\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      5029\n",
      "           1       0.99      0.99      0.99      8878\n",
      "           2       0.98      0.97      0.98      2419\n",
      "           3       0.96      0.96      0.96      6033\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     22359\n",
      "   macro avg       0.97      0.97      0.97     22359\n",
      "weighted avg       0.97      0.97      0.97     22359\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      4893\n",
      "           1       0.99      0.99      0.99      8972\n",
      "           2       0.98      0.97      0.98      2442\n",
      "           3       0.96      0.96      0.96      6052\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     22359\n",
      "   macro avg       0.97      0.97      0.97     22359\n",
      "weighted avg       0.97      0.97      0.97     22359\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      4889\n",
      "           1       0.99      0.99      0.99      9120\n",
      "           2       0.98      0.97      0.98      2351\n",
      "           3       0.96      0.97      0.96      5999\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     22359\n",
      "   macro avg       0.97      0.97      0.97     22359\n",
      "weighted avg       0.97      0.97      0.97     22359\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      5028\n",
      "           1       0.99      0.99      0.99      8917\n",
      "           2       0.99      0.97      0.98      2450\n",
      "           3       0.96      0.96      0.96      5964\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     22359\n",
      "   macro avg       0.97      0.97      0.97     22359\n",
      "weighted avg       0.97      0.97      0.97     22359\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=0, tol=1e-05, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(xtrain,ytrain_matrix):\n",
    "    classifier2.fit(xtrain[train_index],ytrain_matrix[train_index])\n",
    "    ypred=classifier2.predict(xtrain[test_index])\n",
    "    ytestt=ytrain_matrix[test_index]\n",
    "    print(clrp(ytestt,ypred))\n",
    "classifier2.fit(xtrain,ytrain_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain=vectorizer.transform(article[111795:])\n",
    "dataset2['predicted']=classifier2.predict(xtrain)\n",
    "dataset2['predicted']= dataset2['predicted'].map({0: 'Business', 1: 'Entertainment', 2: 'Health',3: 'Technology' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>262120</td>\n",
       "      <td>Tracy Morgan upgraded to fair condition after ...</td>\n",
       "      <td>actor and comedian tracy morgan has been upgr...</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175132</td>\n",
       "      <td>Smartphones Weigh on Samsung Electronics as Gu...</td>\n",
       "      <td>samsung electronics co ltd on tuesday issued u...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                                              Title  \\\n",
       "0  262120  Tracy Morgan upgraded to fair condition after ...   \n",
       "1  175132  Smartphones Weigh on Samsung Electronics as Gu...   \n",
       "\n",
       "                                             Content      predicted  \n",
       "0   actor and comedian tracy morgan has been upgr...  Entertainment  \n",
       "1  samsung electronics co ltd on tuesday issued u...       Business  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2=dataset2.drop('Title',axis=1)\n",
    "dataset2=dataset2.drop('Content',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv = dataset2.to_csv (r'C:\\Users\\Sarah\\Desktop\\export_dataframe2.csv', index = None, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
